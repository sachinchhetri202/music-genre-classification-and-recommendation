{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q0Mmam-pAVM"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio pytorch-lightning scikit-learn pandas soundfile tensorboard tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjoh5HmspI2b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8oICAgbpLY4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torchvision.models as models\n",
        "from sklearn.metrics import confusion_matrix  # New import for confusion matrix\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import tensorboard\n",
        "import tensorboardX\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "logger = CSVLogger(\"logs\", name=\"my_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jAyEzAzqoT6"
      },
      "outputs": [],
      "source": [
        "# Set torchaudio backend (this call is deprecated but needed for .au files)\n",
        "torchaudio.set_audio_backend(\"soundfile\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIItaFveqq4x"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Step 1: Check GPU availability\n",
        "# ----------------------------\n",
        "print(\"Is GPU available?\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYyoRkXFqtDv"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Step 2: Load dataset from Google Drive\n",
        "# ----------------------------\n",
        "# Adjust the path below if your folder structure is different.\n",
        "dataset_path = '../datasets/GTZAN/'\n",
        "file_paths = []\n",
        "genres = []\n",
        "for root, _, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.au'):\n",
        "            file_paths.append(os.path.join(root, file))\n",
        "            genres.append(os.path.basename(root))  # Genre inferred from folder name\n",
        "\n",
        "df = pd.DataFrame({'file_path': file_paths, 'genre': genres})\n",
        "print(\"Dataset loaded. Total files:\", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDge4pwwrT1z"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Step 3: Label encoding for genres\n",
        "# ----------------------------\n",
        "global_le = LabelEncoder()\n",
        "global_le.fit(df['genre'])\n",
        "num_classes = len(global_le.classes_)\n",
        "print(\"Number of classes:\", num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMnilddbrVIH"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Step 4: Define the dataset class to compute mel-spectrograms\n",
        "# ----------------------------\n",
        "class GTZANDataset(Dataset):\n",
        "    def __init__(self, df, label_encoder, duration=10, n_mels=128, hop_length=256):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.duration = duration            # seconds of audio to load\n",
        "        self.n_mels = n_mels                # number of mel bins\n",
        "        self.hop_length = hop_length        # hop length for mel-spectrogram\n",
        "        self.sample_rate = 22050            # standard sample rate\n",
        "        self.le = label_encoder             # shared label encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        file_path = row['file_path']\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(file_path)\n",
        "            # Convert to mono if stereo\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "            target_samples = self.sample_rate * self.duration\n",
        "            if waveform.shape[1] > target_samples:\n",
        "                waveform = waveform[:, :target_samples]\n",
        "            else:\n",
        "                waveform = F.pad(waveform, (0, target_samples - waveform.shape[1]))\n",
        "            # Compute mel-spectrogram and convert to decibel scale\n",
        "            mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=self.sample_rate,\n",
        "                n_mels=self.n_mels,\n",
        "                n_fft=2048,\n",
        "                hop_length=self.hop_length\n",
        "            )\n",
        "            mel_spec = mel_transform(waveform)\n",
        "            mel_spec = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
        "            # Rearrange to shape (time_steps, n_mels)\n",
        "            mel_spec = mel_spec.squeeze(0).T\n",
        "            label = torch.tensor(self.le.transform([row['genre']])[0], dtype=torch.long)\n",
        "            return mel_spec, label\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            default_mel_spec = torch.zeros((self.duration * self.sample_rate // self.hop_length + 1, self.n_mels))\n",
        "            return default_mel_spec, torch.tensor(0, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BgT9QuGrarZ"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Step 5: Split the data into training and validation sets\n",
        "# ----------------------------\n",
        "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42, stratify=df['genre'])\n",
        "train_dataset = GTZANDataset(df_train, label_encoder=global_le, duration=10, n_mels=128, hop_length=256)\n",
        "val_dataset = GTZANDataset(df_val, label_encoder=global_le, duration=10, n_mels=128, hop_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIiAcgcYrgqP"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Step 6: Create DataLoaders\n",
        "# ----------------------------\n",
        "batch_size = 8  # Adjust based on your GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vewIgU1MrjQt"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Step 7: Define the Transfer Learning Model using Pretrained EfficientNet-B7\n",
        "# ----------------------------\n",
        "class MusicEfficientNetB7(pl.LightningModule):\n",
        "    def __init__(self, num_classes, learning_rate=1e-3):\n",
        "        super(MusicEfficientNetB7, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.learning_rate = learning_rate\n",
        "        # Load pretrained EfficientNet-B7\n",
        "        self.model = models.efficientnet_b7(pretrained=True)\n",
        "        # Modify the first conv layer to accept 1 channel instead of 3\n",
        "        self.model.features[0][0] = nn.Conv2d(\n",
        "            1,\n",
        "            self.model.features[0][0].out_channels,\n",
        "            kernel_size=self.model.features[0][0].kernel_size,\n",
        "            stride=self.model.features[0][0].stride,\n",
        "            padding=self.model.features[0][0].padding,\n",
        "            bias=False\n",
        "        )\n",
        "        # Replace the classifier to output the correct number of classes\n",
        "        in_features = self.model.classifier[1].in_features\n",
        "        self.model.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, time_steps, n_mels)\n",
        "        x = x.unsqueeze(1)  # Add channel dimension: (batch, 1, time_steps, n_mels)\n",
        "        # Resize to 300x300 to match EfficientNet-B7 input requirements\n",
        "        x = F.interpolate(x, size=(300, 300), mode=\"bilinear\", align_corners=False)\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_start(self):\n",
        "        # Initialize lists to accumulate predictions and targets\n",
        "        self.all_preds = []\n",
        "        self.all_targets = []\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "        # Accumulate predictions and targets\n",
        "        self.all_preds.append(preds)\n",
        "        self.all_targets.append(y)\n",
        "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # Concatenate all predictions and targets\n",
        "        all_preds = torch.cat(self.all_preds)\n",
        "        all_targets = torch.cat(self.all_targets)\n",
        "        cm = confusion_matrix(all_targets.cpu().numpy(), all_preds.cpu().numpy())\n",
        "        print(\"Confusion Matrix:\\n\", cm)\n",
        "        # Log the confusion matrix if the logger supports add_text\n",
        "        if self.logger is not None and hasattr(self.logger.experiment, \"add_text\"):\n",
        "            self.logger.experiment.add_text(\"Confusion Matrix\", str(cm), self.current_epoch)\n",
        "        else:\n",
        "            print(\"Logger does not support add_text. Confusion Matrix not logged.\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3)\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrOmtFLZrng4"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Step 8: Initialize the model\n",
        "# ----------------------------\n",
        "model = MusicEfficientNetB7(num_classes=num_classes, learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kiNRPIbrqeQ"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Step 9: Configure the Trainer with the CSVLogger\n",
        "# ----------------------------\n",
        "trainer = Trainer(\n",
        "    max_epochs=30,  # More epochs for thorough fine-tuning\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "    devices=1,\n",
        "    precision=\"16-mixed\",  # Mixed precision for faster training on GPU\n",
        "    accumulate_grad_batches=4,\n",
        "    log_every_n_steps=10,\n",
        "    logger=logger,\n",
        "    num_sanity_val_steps=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTlJFOcprs5p"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Step 10: Train and validate the model\n",
        "# ----------------------------\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "results = trainer.validate(model, dataloaders=val_loader)\n",
        "print(\"Validation results:\", results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}